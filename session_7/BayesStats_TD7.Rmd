---
title: "Analyses of piracy in salmon-eating Bald eagles"
author: "F. Barraquand"
date: "November 30, 2021"
output:
  html_document:
    highlight: textmate
    theme: united
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
options(width = 300)
knitr::opts_chunk$set(cache = FALSE) 
library(R2jags)
library(knitr)
library(mcmcplots)
library(MASS)
library(jagsUI)

```

# Binomial GLM

The course part does logistic regression. Now we do something akin to an ANOVA, but with binary data (example adapted from R. Mc Elreath's Statistical Rethinking, itself adapted from the example in ``library(MASS)``). 

```{r eagles-exploring data}
data(eagles)
eagles
#P // Size of pirating eagle (L = large, S = small).
#A // Age of pirating eagle (I = immature, A = adult).
#V // Size of victim eagle (L = large, S = small).
 
eagles.data = list(N=nrow(eagles),y=eagles$y,z=eagles$n,
               P=as.numeric(eagles$P)-1,A=as.numeric(eagles$A)-1,
               V=as.numeric(eagles$V)-1)
# m2.data = list(N=nrow(eagles),y=eagles$y,z=eagles$n,P=eagles$P,A=eagles$A,V=eagles$V)

```


We're in a similar situation to the turtles sex, but with categorical explanatory variables

```{r fitting the first model}

cat(file = "eagles.glm.txt","
model {
  # Priors taken from Mc Elreath's Statistial Rethinking 
  alpha ~ dnorm(0, 0.01)    # prior for the mean on logit scale
  beta_P ~ dnorm(0, 0.04)   # size of pirating eagle (S rather than L) 
  beta_A ~ dnorm(0, 0.04)  # age of pirating eagle (I rather than A)
  beta_V ~ dnorm(0, 0.04)   # size of victim (S rather than L) 

  for (i in 1:N){
  y[i] ~ dbin(p[i],z[i])      ## likelihood
   logit(p[i]) <- alpha + beta_P*P[i] + beta_V*V[i] +beta_A*A[i]
  }
  
}
")

```

Fitting the model

```{r fitting-glm}
# Initial values
inits <- function(){list(alpha=rnorm(1,0,1))}
# Parameters to estimate
params <- c("alpha","beta_P","beta_A","beta_V")
# MCMC settings
nc <- 3  ;  ni <- 10000  ;  nb <- 5000  ;  nt <- 10
# Call JAGS, check convergence and summarize posteriors
out <- R2jags::jags(eagles.data, inits, params, "eagles.glm.txt",
            n.thin = nt, n.chains = nc, n.burnin = nb, n.iter = ni)
print(out, dig = 3)     # Bayesian analysis

nc <- 3  ;  ni <- 10000  ;  nb <- 5000  ;  nt <- 10; na<-1000
# Call JAGS, check convergence and summarize posteriors
out2 <- jagsUI::jags(eagles.data, inits, params, "eagles.glm.txt",
            n.thin = nt, n.chains = nc, n.adapt = na, n.burnin = nb, n.iter = ni)
print(out2, dig = 3)     # Bayesian analysis
```

Model checking

```{r checking}
traplot(out) #something wrong
traceplot(out2)
```

There is a [bug in ``R2jags`` for this model, which outputs in the traceplots the burn-in rather than the iterations after burn-in](https://stats.stackexchange.com/questions/45193/r2jags-does-not-remove-the-burn-in-part-sometimes). It is solved by using ``jagsUI`` instead. ``jagsUI`` functions very similarly but:

- requires an [adaptation phase](https://stackoverflow.com/questions/38701100/how-to-interpret-some-syntax-n-adapt-update-in-jags) (not part of the MCMC chain), which sets the parameters of the sampler before burn-in 
- allows parallel computing, which would be useful if we had much larger datasets and complex models

## Interpreting the model

We see the following effects

- Younger pirates are less successful
- Smaller pirates are less successful
- Smaller targets are more often victims of piracy // probability of success decreases with victim size

All reasonable! But the numbers themselves, to be interpreted, require to get back to the probability scale. A more quantitative appraisal of the estimates requires indeed to compute 

\[ \frac{\text{logistic}(\alpha+\beta_X)}{\text{logistic}(\alpha)}\]

which is factor by which the probability of piracy is changed relative to the change in the factor of interest $X$. 

Let's start with the victim: we have 

```{r change-in-prob}
plogis(out2$mean$alpha)
plogis(out2$mean$alpha+out2$mean$beta_V)
plogis(out2$mean$alpha+out2$mean$beta_V)/plogis(out2$mean$alpha)
```

The reference case is P = "L", A = "A", V = "L" corresponding to $\text{logistic}(\alpha)$. Thus the probability of successful piracy when the thief is a large adult and the victim is also large is 79.7%. When the victim is small, we get to 99.8%, almost 1 (an increase in probability by 1.25%), so a small victim gets robbed with certainty by a large and old adult. 

Let's see things differently and start with a reference situation where the attacker is itself immature and small, and the victim is large. 

```{r change-in-prob-bis}
plogis(out2$mean$alpha+out2$mean$beta_P+out2$mean$beta_A)
plogis(out2$mean$alpha+out2$mean$beta_P+out2$mean$beta_A+out2$mean$beta_V)
plogis(out2$mean$alpha+out2$mean$beta_P+out2$mean$beta_A+out2$mean$beta_V)/plogis(out2$mean$alpha+out2$mean$beta_P+out2$mean$beta_A)
```

Now the probability of attack is barely 1.2%. But if the victim is itself small, the probability of success gets up to 66% (a 55-fold increase), which means that successful theft is strongly driven by size. 

Let's take one further example. Both the attacker and the victim are large, and we check whether immaturity of the attacker changes the outcome:

```{r change-in-prob-ter}
plogis(out2$mean$alpha)
plogis(out2$mean$alpha+out2$mean$beta_A)
plogis(out2$mean$alpha+out2$mean$beta_A)/plogis(out2$mean$alpha)
```

Thus the probability gets down from approximately 80% to 56%, a 70% decrease. Another way to look at this is the use of the [odds ratio](https://en.wikipedia.org/wiki/Odds_ratio) which is $\exp(\beta_A)$ for immaturity, here 33%. The odds are the ratio of probability for vs against the event occurring (here, piracy). An odds ratio is a ratio of those ratios; here it says that the odds of successful piracy are lower by one third when the attacker is immature.  

See [Table 2 of the original article by Knight and Skagen (1988) for more probabilities of success corresponding to the different combinations](https://www.jstor.org/stable/1941273)

A little bit of ecological or evolutionary thinking based on these numbers:

- probabilities of success are fairly high when you're bigger and older, so if these probabilities are larger than those of yourself catching fish in the river in the same timeframe (which looks quite likely), it makes sense to invest in piracy. 
- however, and this is something that we do not see in this observational dataset, there are costs -- each attack provides a risk of injury. 

Further thinking

- You can try interactions
- If we wanted to have uncertainties around these probabilities themselves, we should have written them as "Derived quantities" in the JAGS code, and use them as parameters to output. 

## The model we can't do

We have written the model as

$\text{logit}(p_i) = \alpha + \beta_P P_i +\beta_A A_i + \beta_V V_i$

It could have been tempting to write down, by analogy with the centered logistic regression $\text{logit}(p_i) = \gamma(x_i - \bar{x})$, something like

$\text{logit}(p_i) = \mu + \gamma_P (P_i - \mu_P) +\gamma_A (A_i - \mu_A) + \gamma_V (V_i - \mu_V)$

However, that makes little sense since when we develop the model is equivalent to 

$\text{logit}(p_i) = \underbrace{\mu - \gamma_P \mu_P -\gamma_A \mu_A - \gamma_V \mu_V}_{\text{intercept}} + \gamma_P P_i  +\gamma_A A_i + \gamma_V V_i$

Here the intercept is similar to the ``stupid'' example of the course because several parameters can compensate to create the same intercept, so these will likely not converge (or be equal to their priors).

The logistic regression was different because $\text{logit}(p_i) = \gamma(x_i - \bar{x})$ can be transformed into $\text{logit}(p_i) = a+ b x_i$ with $a = -\gamma \bar{x}$ and $b=\gamma$, with the same number of parameters. Often the (a,b) formulation is easier to fit, and used as a standard in logistic regressions in R. 

There are multiple extensions to the logistic regression, sometimes with [asymmetries](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12473) or asymptotes that are not located at one or zero. 

## A model with hierarchical priors (hyperprior)

Next put a hyperprior on $\beta$s. What are we transforming this model into? 

``` {r glmm}
cat(file = "eagles.glmm.txt","
model {

  alpha ~ dnorm(0, 0.01)    # prior for the mean on logit scale
  beta_P ~ dnorm(0, tau_beta)   # size of pirating eagle (S rather than L) 
  beta_A ~ dnorm(0, tau_beta)  # age of pirating eagle (I rather than A)
  beta_V ~ dnorm(0, tau_beta)   # size of victim (S rather than L) 
  sigma_beta ~ dexp(1)
  tau_beta <-pow(sigma_beta,-2)

  for (i in 1:N){
  y[i] ~ dbin(p[i],z[i])      ## likelihood
   logit(p[i]) <- alpha + beta_P*P[i] + beta_V*V[i] +beta_A*A[i]
  }
  
}
")

```

Fitting the model

```{r fitting-glmm}
# Initial values
inits <- function(){list(alpha=rnorm(1,0,1))}
# Parameters to estimate
params <- c("alpha","beta_P","beta_A","beta_V","sigma_beta")
# MCMC settings
nc <- 3  ;  ni <- 10000  ;  nb <- 5000 ;  nt <- 10; na<-1000
# Call JAGS, check convergence and summarize posteriors
out <- jagsUI::jags(eagles.data, inits, params, "eagles.glm.txt",
            n.thin = nt, n.chains = nc, n.burnin = nb, n.iter = ni)
print(out, dig = 3)     # Bayesian analysis
```

Model checking

```{r checking-bis}
traceplot(out)
```

No real shrinkage. There some ``excursions'' in the chains, which is not ideal. Might be improved by a little thinning of the chains. 
